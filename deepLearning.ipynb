{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45844606828872597"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid(-0.1666)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    if x < 0:\n",
    "        return x\n",
    "    elif x == 0:\n",
    "        return 0.5\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un perceptron from scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "donnes = np.array([[1,0],\n",
    "          [1,1],\n",
    "          [0,0],\n",
    "         [0,1] ])\n",
    "resultats = np.array([[0],[1],[0],[0]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Géneration des poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.165955990594852"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(1)\n",
    "\n",
    "w11= random.uniform(-1,1)\n",
    "w11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.12801019571599248"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.seed(2)\n",
    "\n",
    "w21= random.uniform(-1,1)\n",
    "w21"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout du biais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "biais = 1\n",
    "wb =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparamètres\n",
    "tauxaprentissage = 0.1\n",
    "nbepoch = 200"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DRY > Don't repeat yourself\n",
    "1. Somme pondérée\n",
    "2. La fonction d'activation  >  à  tout les metrics, et voir laquelle à le meilleurs score (sigmoïde, relu, tanh, etc...)\n",
    "3. Calcul de l'erreur\n",
    "4. Calcul du gradient descent\n",
    "5. Calcul de la valeur d'ajustement\n",
    "6.  Calcul ddu nouveau poids\n",
    "7. Calcul du Mean Square Errror MSE (Fonction à créer) ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoïd\n",
    "\n",
    "def sigmoid1(x):\n",
    "  return (1/(1 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(x):\n",
    "  return ((np.exp(x)-np.exp(-x)) / (np.exp(x)+np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "  if x > 0 :\n",
    "    return x\n",
    "  elif x == 0:\n",
    "    return 0.5\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sommePonderee(x1,x2,xbiais, poids1, poids2, poidsbiais):\n",
    "    sommePond = x1 *poids1 + x2*poids2 + xbiais*poidsbiais\n",
    "    return sommePond\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRY > Don't repeat yourself\n",
    "# Somme pondérée\n",
    "\n",
    "# La fonction d'activation  >  à  tout les metrics, et voir laquelle à le meilleurs score (sigmoïde, relu, tanh, etc...)\n",
    "def fonctionActivation(nom, sommePonderee):\n",
    "    if nom == \"sigmoïde\":\n",
    "        #fct sigmoïde\n",
    "        Activation = sigmoid1(sommePonderee)\n",
    "    elif nom == \"ReLu\":\n",
    "        #fct Relu\n",
    "        Activation = relu(sommePonderee)\n",
    "    elif nom == \"tanh\":\n",
    "        #fct TanH\n",
    "        Activation = tanh(sommePonderee)\n",
    "    else:\n",
    "        #fct softmax\n",
    "        Activation = softmax(sommePonderee)\n",
    "    \n",
    "    return Activation\n",
    "        \n",
    "# Calcul de l'erreur\n",
    "# Calcul du gradient descent\n",
    "# Calcul de la valeur d'ajustement\n",
    "# Calcul ddu nouveau poids\n",
    "def Ajustement_poids(erreur, predictions_realisees, val_input):\n",
    "    gradient = -1 * erreur * (1- predictions_realisees) * val_input\n",
    "    return gradient\n",
    "# Calcul du Mean Square Errror MSE (Fonction à créer) \n",
    "\n",
    "# def calcul_MSE(predictions_realisees, predictions_attendues): \n",
    "#    i=0 \n",
    "#    for _ in predictions_attendues: \n",
    "#        difference = predictions_attendues[i] - predictions_realisees[i] \n",
    "#        carreDifference = difference * difference \n",
    "#        somme = somme + carreDifference \n",
    "#        i += 1\n",
    "#    moyenne_quadratique = 1 / (len(predictions_attendues)) * somme \n",
    "#    return moyenne_quadratique \n",
    "\n",
    "\n",
    "def calcul_MSE(predictions_realisees, predictions_attendues): \n",
    "    return np.square(np.subtract(predictions_attendues, predictions_realisees)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 \\ 201\n",
      "x1 , x2 - [1 0] pred_attendue - [0]\n",
      "EPOCH 1 \\ 201 - Observation: 1 \\ 4\n",
      "erreur - 0.16444903692853854\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'MSE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_41477/408207724.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"erreur - {erreur}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mstockagepoiDesInfos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoids1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoids2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxbiais\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpoidsbiais\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_realisees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMSE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# Màj du poids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mgradient1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAjustement_poids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merreur\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions_realisees\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MSE' is not defined"
     ]
    }
   ],
   "source": [
    "# On se servira \n",
    "list_epoch= []\n",
    "list_MSE = []\n",
    "for epoch in range(1, nbepoch+1):\n",
    "    i=0 \n",
    "    print(f\"EPOCH {epoch} \\ {nbepoch+1}\")\n",
    "    # boucler sur chaque lign de data\n",
    "    x1 = 1\n",
    "    x2 = 0\n",
    "    xbiais = 1\n",
    "    poidsbiais = 0\n",
    "    random.seed(1)\n",
    "    poids1 = random.uniform(-1, 1)\n",
    "    random.seed(2)\n",
    "    poids2 = random.uniform(-1,1)\n",
    "\n",
    "    taux_apprentissage = 0.1\n",
    "    \n",
    "    \n",
    "    for ligne, prediction_attendu in zip(donnes, resultats):\n",
    "        x1 = ligne[0]\n",
    "        x2 = ligne[1]\n",
    "        print(f\"x1 , x2 - {ligne} pred_attendue - {prediction_attendu}\" )\n",
    "        print(f\"EPOCH {epoch} \\ {nbepoch+1} - Observation: {i+1} \\ {len(donnes)}\")\n",
    "\n",
    "\n",
    "        sommePond = sommePonderee(x1, x2, xbiais, poids1, poids2, poidsbiais)\n",
    "        predictions_realisees = fonctionActivation(\"tanh\", sommePond)\n",
    "        erreur = prediction_attendu[0] - predictions_realisees\n",
    "        print(f\"erreur - {erreur}\")\n",
    "\n",
    "        stockagepoiDesInfos = [x1, poids1, x2, poids2, xbiais, poidsbiais, predictions_realisees, epoch, MSE]\n",
    "        # Màj du poids\n",
    "        gradient1 = Ajustement_poids(erreur, predictions_realisees, x1)\n",
    "        gradient2 = Ajustement_poids(erreur, predictions_realisees, x2)\n",
    "        gradientbiais = Ajustement_poids(erreur, predictions_realisees, xbiais)\n",
    "        ajustementPoids1 = gradient1 * taux_apprentissage\n",
    "        ajustementPoids2 = gradient2 * taux_apprentissage\n",
    "        ajustementPoidsbiais = gradientbiais * taux_apprentissage\n",
    "        poids1 = poids1 - ajustementPoids1 \n",
    "        poids2 = poids2 - ajustementPoids2 \n",
    "        poidsbiais = poidsbiais - ajustementPoidsbiais\n",
    "\n",
    "        \n",
    "        MSE = calcul_MSE(predictions_realisees, prediction_attendu)\n",
    "        i += 1\n",
    "        print(f\"MSE = {MSE }\")\n",
    "        list_MSE.append(MSE)\n",
    "    list_epoch.append(epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899,\n",
       " 0.03503411680428899]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for epoch, mse in zip(list_epoch,list_MSE):\n",
    "# list_epoch\n",
    "list_MSE\n",
    "# plt.plot(list_epoch, list_MSE)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
